{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from Resnet_from_scratch import ResNet18, ResNet34, ResNet50, ResNet101, ResNet152\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics.functional as functional\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "class VitLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-3, num_classes=11):\n",
    "        super(VitLightningModule, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.validation_step_y_hats = []\n",
    "        self.validation_step_ys = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = self.accuracy(y_hat.softmax(dim=-1), y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = self.accuracy(y_hat.softmax(dim=-1), y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = self.accuracy(y_hat.softmax(dim=-1), y)\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', acc)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.curdir, \"data\",\"raw\",\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preprocess = transforms.Compose([\\n    transforms.Resize((224, 224)),\\n    transforms.ToTensor(),          \\n    transforms.Normalize(           \\n        mean=[0.485, 0.456, 0.406],\\n        std=[0.229, 0.224, 0.225],\\n    ),\\n])\\n\\ndata_dir = os.path.join(os.curdir, \"data\", \"raw\",\"dataset\")\\ndataset = datasets.ImageFolder(data_dir, transform=preprocess)\\n\\ntrain_size = int(TRAIN_SIZE * len(dataset))\\nval_size = int(0.1 * len(dataset))\\ntest_size = len(dataset) - train_size - val_size\\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\\n\\n# need to expirement with number of workers in each dataloader\\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),          \n",
    "    transforms.Normalize(           \n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "data_dir = os.path.join(os.curdir, \"data\", \"raw\",\"dataset\")\n",
    "dataset = datasets.ImageFolder(data_dir, transform=preprocess)\n",
    "\n",
    "train_size = int(TRAIN_SIZE * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# need to expirement with number of workers in each dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-4 ,num_classes=11):\n",
    "        super(ResNetLightningModule, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ])\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes = num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = self.accuracy(y_hat.softmax(dim=-1), y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = self.accuracy(y_hat.softmax(dim=-1), y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = self.accuracy(y_hat.softmax(dim=-1), y)\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size=32, train_size=0.8):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.train_size = train_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = datasets.ImageFolder(self.data_dir, transform=self.transform)\n",
    "        train_size = int(self.train_size * len(dataset))\n",
    "        val_size = int(0.1 * len(dataset))\n",
    "        test_size = len(dataset) - train_size - val_size\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(logger):\n",
    "    metrics_path = os.path.join(logger.save_dir, logger.name, f\"version_{logger.version}\", \"metrics.csv\")\n",
    "    \n",
    "    if not os.path.exists(metrics_path):\n",
    "        raise FileNotFoundError(f\"Metrics file not found at: {metrics_path}\")\n",
    "\n",
    "    metrics = pd.read_csv(metrics_path)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    train_loss = metrics.dropna(subset=['train_loss'])\n",
    "    val_loss = metrics.dropna(subset=['val_loss'])\n",
    "    \n",
    "    plt.plot(train_loss['epoch'], train_loss['train_loss'], label='Training Loss')\n",
    "    plt.plot(val_loss['epoch'], val_loss['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | ResNet             | 11.2 M | train\n",
      "1 | criterion | CrossEntropyLoss   | 0      | train\n",
      "2 | accuracy  | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.748    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44942c4c1894dd99911b0fe406619eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mshaker/miniconda3/envs/res/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "/home/mshaker/miniconda3/envs/res/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2f3a65aa904a32ab933c7a053937d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    model = ResNet18(num_classes = 11, channels = 3)\n",
    "    pl_model = ResNetLightningModule(model)\n",
    "\n",
    "    data_dir = os.path.join(os.curdir, \"data\", \"raw\", \"dataset\")\n",
    "    data_module = ImageFolderDataModule(data_dir)\n",
    "\n",
    "    csv_logger = pl_loggers.CSVLogger('logs/', name='csv_logs')\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=4, mode=\"min\")\n",
    "\n",
    "    trainer = pl.Trainer(callbacks=[early_stop_callback] ,max_epochs = 15 , devices = 1, accelerator='gpu', logger = csv_logger)\n",
    "    trainer.fit(pl_model, data_module)\n",
    "    trainer.test(pl_model, data_module)\n",
    "\n",
    "    plot_loss_curves(csv_logger)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "res",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
