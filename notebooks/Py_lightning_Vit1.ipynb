{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import lightning.pytorch as pl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models.vision_transformer import vit_b_16\n",
    "from torchvision.models import ViT_B_16_Weights\n",
    "from PIL import Image\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.curdir, \"data\", \"raw\", \"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1.2e-4, num_classes=11):\n",
    "        super(VitLightningModule, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.transform = ViT_B_16_Weights.DEFAULT.transforms()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = self.accuracy(y_hat.softmax(dim=-1), y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = self.accuracy(y_hat.softmax(dim=-1), y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc = self.accuracy(y_hat.softmax(dim=-1), y)\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', acc)\n",
    "\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size=32, train_size=0.8):\n",
    "        super().__init__()\n",
    "        self.data_dir = DATA_DIR\n",
    "        self.batch_size = batch_size\n",
    "        self.train_size = train_size\n",
    "        self.transform = ViT_B_16_Weights.DEFAULT.transforms()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = datasets.ImageFolder(self.data_dir, transform=self.transform)\n",
    "        train_size = int(self.train_size * len(dataset))\n",
    "        val_size = int(0.1 * len(dataset))\n",
    "        test_size = len(dataset) - train_size - val_size\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True , num_workers=31)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=31)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(logger):\n",
    "    metrics_path = os.path.join(logger.save_dir, logger.name, f\"version_{logger.version}\", \"metrics.csv\")\n",
    "    \n",
    "    if not os.path.exists(metrics_path):\n",
    "        raise FileNotFoundError(f\"Metrics file not found at: {metrics_path}\")\n",
    "\n",
    "    metrics = pd.read_csv(metrics_path)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    train_loss = metrics.dropna(subset=['train_loss'])\n",
    "    val_loss = metrics.dropna(subset=['val_loss'])\n",
    "    \n",
    "    plt.plot(train_loss['epoch'], train_loss['train_loss'], label='Training Loss')\n",
    "    plt.plot(val_loss['epoch'], val_loss['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | model     | VisionTransformer   | 85.8 M | train\n",
      "1 | criterion | CrossEntropyLoss    | 0      | train\n",
      "2 | accuracy  | MulticlassAccuracy  | 0      | train\n",
      "3 | transform | ImageClassification | 0      | train\n",
      "----------------------------------------------------------\n",
      "85.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "85.8 M    Total params\n",
      "343.228   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7581d1f82de47599ab140002d21ea3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2f99d3aa01430db7bf4d8f1aafe9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad93d6b0076b404f97e6d212143566c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6747492c3f47e3bc441a2d361ed72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "model.heads.head = nn.Linear(model.heads.head.in_features, 11)\n",
    "\n",
    "pl_model = VitLightningModule(model)\n",
    "\n",
    "data_module = ImageFolderDataModule(DATA_DIR, batch_size=BATCH_SIZE, train_size=TRAIN_SIZE)\n",
    "\n",
    "csv_logger = pl_loggers.CSVLogger('logs/', name='csv_logs')\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.05, patience=4, mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(callbacks=[early_stop_callback], max_epochs = 120, devices = 1, accelerator='gpu', logger = csv_logger, log_every_n_steps=10)\n",
    "\n",
    "#very time consuming to find optimal batch size and learning rate, just use first time and then hardcode it\n",
    "'''\n",
    "# finding optimal batch size\n",
    "# maximum batch size was : 128, but takes too long to train so using 32 instead for fast testing purposes\n",
    "tuner = Tuner(trainer)\n",
    "new_batch_size = tuner.scale_batch_size(pl_model, datamodule=data_module, mode=\"power\")\n",
    "\n",
    "# Update data module with new batch size\n",
    "data_module.batch_size = new_batch_size\n",
    "\n",
    "# Find the optimal learning rate\n",
    "# optimal learning rate was 0.00012022644346174131\n",
    "lr_finder = tuner.lr_find(pl_model, datamodule=data_module)\n",
    "new_lr = lr_finder.suggestion()\n",
    "pl_model.learning_rate = new_lr\n",
    "'''\n",
    "    \n",
    "\n",
    "    \n",
    "trainer.fit(pl_model, data_module)\n",
    "trainer.test(pl_model, data_module)\n",
    "\n",
    "plot_loss_curves(csv_logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = data_module.test_dataloader()\n",
    "\n",
    "test_model = pl_model.model\n",
    "test_model.eval()\n",
    "\n",
    "def compute_metrics(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    TP = np.diag(cm)\n",
    "    FP = cm.sum(axis=0) - TP\n",
    "    FN = cm.sum(axis=1) - TP\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "    return cm, all_labels, all_preds, TP, FP, FN, TN\n",
    "\n",
    "def calculate_macro_micro_f1(all_labels, all_preds):\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    micro_f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "    return macro_f1, micro_f1\n",
    "\n",
    "conf_matrix, all_labels, all_preds, TP, FP, FN, TN = compute_metrics(test_model, test_dataloader)\n",
    "\n",
    "cm_df = pd.DataFrame(conf_matrix, index =[i for i in ['dew', 'fogsmog', 'frost', 'glaze', 'hail', 'lightning', 'rain', 'rainbow', 'rime', 'sandstorm', 'snow']],\n",
    "                     columns=[i for i in ['dew', 'fogsmog', 'frost', 'glaze', 'hail', 'lightning', 'rain', 'rainbow', 'rime', 'sandstorm', 'snow']])\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "accuracy = (TP.sum() + TN.sum()) / (TP.sum() + TN.sum() + FP.sum() + FN.sum())\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1_score_per_class = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "macro_f1, micro_f1 = calculate_macro_micro_f1(all_labels, all_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision per class: {precision}\")\n",
    "print(f\"Recall per class: {recall}\")\n",
    "print(f\"F1 Score per class: {f1_score_per_class}\")\n",
    "print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "print(f\"Micro F1 Score: {micro_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "res",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
